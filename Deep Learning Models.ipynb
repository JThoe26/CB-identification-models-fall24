{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0dc623000235>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Bidirectional, LSTM, GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('labeled_comments.csv')\n",
    "\n",
    "# Ensure comments are strings and handle NaN values\n",
    "df['Comment'] = df['Comment'].astype(str).fillna('')\n",
    "\n",
    "# Tokenization and Preprocessing\n",
    "max_words = 10000 \n",
    "max_len = 100      \n",
    "\n",
    "# Using Keras Tokenizer to vectorize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['Comment'])\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df['Comment'])\n",
    "\n",
    "# Pad the sequences to ensure uniform input length\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Encode the labels (if they are not binary, you can adjust this for multiclass classification)\n",
    "df['Label'] = df['Label'].astype(str) \n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# CNN Model Architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer: Converts words to dense vectors of fixed size\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "\n",
    "# Convolutional layer with a kernel size of 5 and 128 filters\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "\n",
    "# Pooling layer: Reduces the dimensionality\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Dense fully connected layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout to reduce overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer: Binary classification (you can adjust for more classes)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {val_acc}')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "print(f'Classification Report:\\n{classification_report(y_val, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM Model Architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer: Converts words to dense vectors of fixed size\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "\n",
    "# layer with a kernel 128 filters\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "\n",
    "# Dense fully connected layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout to reduce overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer: Binary classification (you can adjust for more classes)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {val_acc}')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "print(f'Classification Report:\\n{classification_report(y_val, y_pred)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiGRU Model Architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer: Converts words to dense vectors of fixed size\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "\n",
    "# layer with a kernel 128 filters\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
    "\n",
    "model.add(Bidirectional(GRU(32)))\n",
    "\n",
    "# Dense fully connected layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Dropout to reduce overfitting\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer: Binary classification (you can adjust for more classes)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model on validation data\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Accuracy: {val_acc}')\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "print(f'Classification Report:\\n{classification_report(y_val, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('data/labeled_comments.csv')\n",
    "\n",
    "df['Comment'] = df['Comment'].astype(str).fillna('')\n",
    "\n",
    "# Add a dummy label column to start with (you'll replace this later after labeling)\n",
    "df['Label'] = 0  \n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['Comment'], df['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_data(texts):\n",
    "    return tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "train_encodings = tokenize_data(train_texts)\n",
    "val_encodings = tokenize_data(val_texts)\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_labels.values)\n",
    "val_labels = torch.tensor(val_labels.values)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "\n",
    "# Use DataLoader for batching\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_predictions, val_labels_list = [], []\n",
    "    for batch in val_loader:\n",
    "        batch_input_ids, batch_attention_mask, batch_labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        val_predictions.extend(predictions)\n",
    "        val_labels_list.extend(batch_labels.cpu().numpy())\n",
    "    return accuracy_score(val_labels_list, val_predictions), classification_report(val_labels_list, val_predictions)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3 \n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} | Training Loss: {train_loss}')\n",
    "    val_accuracy, val_report = evaluate(model, val_loader)\n",
    "    print(f'Validation Accuracy: {val_accuracy}')\n",
    "    print(val_report)\n",
    "# # Predict on all comments (label the dataset)\n",
    "# def predict_comments(model, comments):\n",
    "#     encodings = tokenize_data(comments)\n",
    "#     input_ids = encodings['input_ids']\n",
    "#     attention_mask = encodings['attention_mask']\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#     logits = outputs.logits\n",
    "#     predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "#     return predictions\n",
    "\n",
    "# # Apply the model to the entire dataset\n",
    "# all_comments = df['Comment']\n",
    "# df['Label'] = predict_comments(model, all_comments)\n",
    "# df.head()\n",
    "\n",
    "# Save the labeled dataset to a new CSV\n",
    "#df.to_csv('labeled_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
